{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1212, 832) (1212, 1) (776, 832)\n"
     ]
    }
   ],
   "source": [
    "# Import the train and test data\n",
    "df_X_train = pd.read_csv('X_train.csv').iloc[:, 1:]\n",
    "df_y_train = pd.read_csv('y_train.csv').iloc[:, 1:]\n",
    "df_X_test = pd.read_csv('X_test.csv')\n",
    "\n",
    "X_train = df_X_train.to_numpy()\n",
    "y_train = df_y_train.to_numpy()\n",
    "X_test = df_X_test.to_numpy()\n",
    "id_test = X_test[:, 0]\n",
    "X_test = X_test[:, 1:]\n",
    "\n",
    "print(X_train.shape, y_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "(1212, 832) (1212, 1) (776, 832)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Imput missing values using the mean of each column (basic : try to find more pertinent)\n",
    "\n",
    "# imput missing values using the k-neighbors imputer (more advanced)\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "# Create the imputer object, with 10 neighbors\n",
    "imputer = KNNImputer(n_neighbors=10, weights='distance')\n",
    "\n",
    "# Fit the imputer object on the train data\n",
    "imputer.fit(X_train)\n",
    "\n",
    "# Impute the missing values on the train and test data\n",
    "X_train = imputer.transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Check that there is no more missing values    \n",
    "print(np.isnan(X_train).sum(), np.isnan(X_test).sum())\n",
    "print(X_train.shape, y_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1212, 666) (1212, 1) (776, 666)\n"
     ]
    }
   ],
   "source": [
    "# Remove features with low variance\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=0.1)  # remove features with more than 80% variance\n",
    "X_train = sel.fit_transform(X_train, y_train)\n",
    "X_test = sel.transform(X_test)\n",
    "print(X_train.shape, y_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of relevant features :  631\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 185, 186, 187, 188, 189, 190, 192, 193, 194, 195, 196, 197, 198, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 225, 226, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 407, 408, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 453, 454, 455, 456, 457, 459, 460, 461, 462, 463, 464, 465, 466, 467, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 493, 494, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 535, 536, 537, 538, 539, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 620, 621, 622, 623, 625, 626, 627, 628, 629, 630, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665]\n"
     ]
    }
   ],
   "source": [
    "### Drop highly correlated features\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming that X_train is your ndarray and it only contains feature columns\n",
    "df = pd.DataFrame(X_train)\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Create a set to hold the correlated columns\n",
    "corr_columns = set()\n",
    "\n",
    "# Iterate over the correlation matrix\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        # If the correlation between the columns is high, add it to the set\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.9:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            corr_columns.add(colname)\n",
    "\n",
    "# Get the indices of the relevant features\n",
    "relevant_features = [df.columns.get_loc(c) for c in df.columns if c not in corr_columns]\n",
    "\n",
    "X_train = X_train[:, relevant_features]\n",
    "X_test = X_test[:, relevant_features]\n",
    "# Print the relevant feature indices\n",
    "print(\"Number of relevant features : \", X_train.shape[1])\n",
    "print(relevant_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1212, 90) (776, 90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mchami/ETH/AML/Task1/AgePredictionProject/.env/lib/python3.11/site-packages/sklearn/utils/validation.py:1310: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Select the most relevant features\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Create the SelectKBest with the mutual info strategy\n",
    "selector = SelectKBest(f_regression, k=90)\n",
    "\n",
    "# Fit the object to the training data\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Transform the data\n",
    "X_train = selector.transform(X_train)\n",
    "X_test = selector.transform(X_test)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers removed:  65\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Define the Isolation Forest model\n",
    "clf = IsolationForest(random_state=42, n_estimators=500)\n",
    "# Fit the model\n",
    "clf.fit(X_train)\n",
    "\n",
    "# Predict the outliers\n",
    "outliers = clf.predict(X_train)\n",
    "\n",
    "# Remove the outliers\n",
    "mask = outliers != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "\n",
    "print('Number of outliers removed: ', sum(outliers == -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Split the data into training and testing sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((917, 90), (917, 1), (230, 90), (230, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mchami/ETH/AML/Task1/AgePredictionProject/.env/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:970: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/mchami/ETH/AML/Task1/AgePredictionProject/task1.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mchami/ETH/AML/Task1/AgePredictionProject/task1.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m reg \u001b[39m=\u001b[39m StackingRegressor(estimators\u001b[39m=\u001b[39mestimators, verbose\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mchami/ETH/AML/Task1/AgePredictionProject/task1.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Fit the model to the training data\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mchami/ETH/AML/Task1/AgePredictionProject/task1.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m reg\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mchami/ETH/AML/Task1/AgePredictionProject/task1.ipynb#X16sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Predict the test data\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mchami/ETH/AML/Task1/AgePredictionProject/task1.ipynb#X16sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m y_pred \u001b[39m=\u001b[39m reg\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/ETH/AML/Task1/AgePredictionProject/.env/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:971\u001b[0m, in \u001b[0;36mStackingRegressor.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    969\u001b[0m _raise_for_unsupported_routing(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m\"\u001b[39m, sample_weight\u001b[39m=\u001b[39msample_weight)\n\u001b[1;32m    970\u001b[0m y \u001b[39m=\u001b[39m column_or_1d(y, warn\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 971\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(X, y, sample_weight)\n",
      "File \u001b[0;32m~/ETH/AML/Task1/AgePredictionProject/.env/lib/python3.11/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/ETH/AML/Task1/AgePredictionProject/.env/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:224\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mappend(estimator)\n\u001b[1;32m    220\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[39m# Fit the base estimators on the whole training data. Those\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[39m# base estimators will be used in transform, predict, and\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[39m# predict_proba. They are exposed publicly.\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_ \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs)(\n\u001b[1;32m    225\u001b[0m         delayed(_fit_single_estimator)(clone(est), X, y, fit_params)\n\u001b[1;32m    226\u001b[0m         \u001b[39mfor\u001b[39;49;00m est \u001b[39min\u001b[39;49;00m all_estimators\n\u001b[1;32m    227\u001b[0m         \u001b[39mif\u001b[39;49;00m est \u001b[39m!=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mdrop\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    228\u001b[0m     )\n\u001b[1;32m    230\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnamed_estimators_ \u001b[39m=\u001b[39m Bunch()\n\u001b[1;32m    231\u001b[0m est_fitted_idx \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/ETH/AML/Task1/AgePredictionProject/.env/lib/python3.11/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/ETH/AML/Task1/AgePredictionProject/.env/lib/python3.11/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[39m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n",
      "File \u001b[0;32m~/ETH/AML/Task1/AgePredictionProject/.env/lib/python3.11/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/ETH/AML/Task1/AgePredictionProject/.env/lib/python3.11/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.01\u001b[39m)\n\u001b[1;32m   1763\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Define the base models\n",
    "estimators = [\n",
    "    ('rf', RandomForestRegressor(max_depth=None, min_samples_split=2, min_samples_leaf=1, n_estimators=500, n_jobs=-1, random_state=0, criterion='absolute_error')),\n",
    "    ('et', ExtraTreesRegressor(max_depth=None, min_samples_split=2, min_samples_leaf=1, n_estimators=500, n_jobs=-1, random_state=42)),\n",
    "    ('dt', DecisionTreeRegressor(max_depth=3, min_samples_split=2, min_samples_leaf=1, random_state=0)),\n",
    "    ('et2', ExtraTreesRegressor(max_depth=None, min_samples_split=10, min_samples_leaf=9, n_estimators=300, n_jobs=-1, random_state=42)),\n",
    "    ('rf2', RandomForestRegressor(max_depth=20, min_samples_split=2, min_samples_leaf=1, n_estimators=200, n_jobs=-1, random_state=0, criterion='absolute_error')),\n",
    "    ('et3', ExtraTreesRegressor(max_depth=None, min_samples_split=2, min_samples_leaf=1, n_estimators=500, n_jobs=-1, random_state=42)),\n",
    "    ('rf3', RandomForestRegressor(max_depth=20, min_samples_split=2, min_samples_leaf=1, n_estimators=200, n_jobs=-1, random_state=0, criterion='absolute_error')),\n",
    "    ('dt2', DecisionTreeRegressor(max_depth=3, min_samples_split=2, min_samples_leaf=1, random_state=0)),\n",
    "]\n",
    "\n",
    "# Define the meta model\n",
    "\n",
    "# Define the stacking regressor\n",
    "reg = StackingRegressor(estimators=estimators, verbose=3, n_jobs=-1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test data\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# Calculate the R2 score for training and validation data\n",
    "pred_test = reg.predict(X_val)\n",
    "training_test = reg.predict(X_train)\n",
    "\n",
    "train_sc = r2_score(y_train, training_test)\n",
    "val_sc = r2_score(y_test, pred_test)\n",
    "train_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import ElasticNet\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Create the ElasticNet model\n",
    "# # model = ElasticNet(alpha=5, l1_ratio=1, random_state=0)\n",
    "# # model = DecisionTreeRegressor(max_depth=3, min_samples_split=2, min_samples_leaf=1, random_state=0)\n",
    "# model = RandomForestRegressor(max_depth=15, \n",
    "#                               min_samples_split=2, \n",
    "#                               min_samples_leaf=1, \n",
    "#                               n_estimators=200, \n",
    "#                               n_jobs=-1, \n",
    "#                               random_state=0,\n",
    "#                               criterion='absolute_error',\n",
    "#                               )\n",
    "# # model = ExtraTreesRegressor(max_depth=None, \n",
    "# #                             min_samples_split=10, \n",
    "# #                             min_samples_leaf=9, \n",
    "# #                             n_estimators=3000, \n",
    "# #                             n_jobs=-1, \n",
    "# #                             random_state=42)\n",
    "# # Fit the model to the training data\n",
    "\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Predict the test data\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# pred_test = model.predict(X_val)\n",
    "# training_test = model.predict(X_train)\n",
    "\n",
    "# train_sc = r2_score(y_train, training_test)\n",
    "# val_sc = r2_score(y_test, pred_test)\n",
    "# train_sc, val_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict the test data and output it to a file \"out.csv\"\n",
    "y_out = reg.predict(X_test)\n",
    "output = np.stack((id_test, y_out.flatten()), axis=-1)\n",
    "df_out = pd.DataFrame(output, columns=[\"id\", \"y\"])\n",
    "\n",
    "df_out.to_csv(\"out_rg.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advancedmachinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
